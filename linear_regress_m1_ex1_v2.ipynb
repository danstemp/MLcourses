{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Welcome to your first assignment. This exercise gives you a brief introduction to linear regression. The exercise is to be implemented in Python. Even if you've used Python before, this will help familiarize you with functions we'll need.  \n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
    "- Do not modify the (# GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to implement linear regression model using statsmodels, scikit-learn, and tensorflow\n",
    "- Work with simulated non-linear dataset\n",
    "- Compare model performance (quality of fit) of both models\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(≈ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except: pass\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as core_layers\n",
    "try:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    \"\"\"\n",
    "    Utility function to reset current tensorflow computation graph\n",
    "    and set the random seed \n",
    "    \"\"\"\n",
    "    # to make results reproducible across runs\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## We use artificial data for the following two specifications of regression:\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "$ y(x) = a + b_1 \\cdot X_1 + b_2 \\cdot X_2 + b_3 \\cdot X_3 + \\sigma \\cdot \\varepsilon $ \n",
    "\n",
    "where $ \\varepsilon \\sim N(0, 1) $ is a Gaussian noise, and $ \\sigma $ is its volatility, \n",
    "with the following choice of parameters:\n",
    "\n",
    "$ a = 1.0 $\n",
    "\n",
    "$ b_1, b_2, b_3 = (0.5, 0.2, 0.1) $\n",
    "\n",
    "$ \\sigma = 0.1 $\n",
    "\n",
    "$ X_1, X_2, X_3 $ will be uniformally distributed in $ [-1,1] $\n",
    "\n",
    "### Non-Linear Regression\n",
    "\n",
    "$ y(x) = a + w_{00} \\cdot X_1 + w_{01} \\cdot X_2 + w_{02} \\cdot X_3 + + w_{10} \\cdot X_1^2 \n",
    "+ w_{11} \\cdot X_2^2 + w_{12} \\cdot X_3^2 +  \\sigma \\cdot \\varepsilon $ \n",
    "\n",
    "where\n",
    "\n",
    "$ w = [[1.0, 0.5, 0.2],[0.5, 0.3, 0.15]]  $\n",
    "\n",
    "and the rest of parameters is as above, with the same values of $ X_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 3), (7500, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data(n_points=10000, n_features=3, use_nonlinear=True, \n",
    "                    noise_std=0.1, train_test_split = 4):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_points - number of data points to generate\n",
    "    n_features - a positive integer - number of features\n",
    "    use_nonlinear - if True, generate non-linear data\n",
    "    train_test_split - an integer - what portion of data to use for testing\n",
    "    \n",
    "    Return:\n",
    "    X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Linear data or non-linear data?\n",
    "    if use_nonlinear:\n",
    "        weights = np.array([[1.0, 0.5, 0.2],[0.5, 0.3, 0.15]])\n",
    "    else:\n",
    "        weights = np.array([1.0, 0.5, 0.2])\n",
    "        \n",
    "    bias = np.ones(n_points).reshape((-1,1))\n",
    "    low = - np.ones((n_points,n_features),'float')\n",
    "    high = np.ones((n_points,n_features),'float')\n",
    "    \n",
    "    X = np.random.uniform(low=low, high=high)\n",
    "    noise = np.random.normal(size=(n_points, 1))\n",
    "    noise_std = 0.1\n",
    "    \n",
    "    if use_nonlinear:\n",
    "        Y = (weights[0,0] * bias + np.dot(X, weights[0, :]).reshape((-1,1)) + \n",
    "             np.dot(X*X, weights[1, :]).reshape([-1,1]) +\n",
    "             noise_std * noise)\n",
    "    else:\n",
    "        Y = (weights[0] * bias + np.dot(X, weights[:]).reshape((-1,1)) + \n",
    "             noise_std * noise)\n",
    "    \n",
    "    n_test = int(n_points/train_test_split)\n",
    "    n_train = n_points - n_test\n",
    "    \n",
    "    X_train = X[:n_train,:]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, n_train, n_features = generate_data(use_nonlinear=False)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: numpy_lin_regress\n",
    "def numpy_lin_regress(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    numpy_lin_regress - Implements linear regression model using numpy module\n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return:\n",
    "    np.array of size (k+1 by 1) of regression coefficients\n",
    "    \"\"\"\n",
    "    # number of features\n",
    "    ndim = X_train.shape[1] \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    design_matrix = np.insert(X_train, 0, [1.], axis=1)\n",
    "    theta_numpy = np.linalg.lstsq(design_matrix,Y_train)[0]  # replace this line with your calculation\n",
    "    ### END CODE HERE ###\n",
    "    return theta_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00162408,  0.99816368,  0.5042376 ,  0.20078698])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "theta_numpy = numpy_lin_regress(X_train, Y_train)\n",
    "theta_numpy.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sklearn_lin_regress\n",
    "def sklearn_lin_regress(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return:\n",
    "    np.array of size (k+1 by 1) of regression coefficients\n",
    "    \"\"\"\n",
    "    ndim = X_train.shape[1] \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    model = linear_model.LinearRegression()\n",
    "    fitted_model = model.fit(X_train, Y_train)\n",
    "    print(\"The model intercept is \", fitted_model.intercept_)\n",
    "    print(\"The model coefficients are: \",fitted_model.coef_)\n",
    "    theta_sklearn = np.insert(fitted_model.coef_, 0, fitted_model.intercept_,axis=1) # replace this line with your calculation\n",
    "    ### END CODE HERE ###\n",
    "    return theta_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model intercept is  [ 1.00162408]\n",
      "The model coefficients are:  [[ 0.99816368  0.5042376   0.20078698]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00162408,  0.99816368,  0.5042376 ,  0.20078698])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "theta_sklearn = sklearn_lin_regress(X_train, Y_train)\n",
    "theta_sklearn.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: tf_lin_regress\n",
    "def tf_lin_regress(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return:\n",
    "    np.array of size (k+1 by 1) of regression coefficients\n",
    "    \"\"\"\n",
    "    ndim = X_train.shape[1] \n",
    "    ### START CODE HERE ### (≈ 7-8 lines of code)\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[3])]\n",
    "    model = tf.estimator.LinearRegressor(feature_columns=feature_columns) \n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": X_train}, Y_train, shuffle=True)\n",
    "    \n",
    "    #train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    #{\"x\": X_train}, Y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "    #eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    #{\"x\": X_test}, Y_test, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "    \n",
    "    model.train(input_fn=input_fn, steps=1000000)\n",
    "    #print(model.get_variable_names()) \n",
    "    variables = []\n",
    "    for i in [0,1,4]:\n",
    "        variable_name = model.get_variable_names()[i]\n",
    "        variable_value = model.get_variable_value(variable_name)\n",
    "        print('{0}: {1}'.format(variable_name,variable_value))\n",
    "        variables.append(variable_value)\n",
    "    theta_value = np.insert(model.get_variable_value(model.get_variable_names()[4]),0,model.get_variable_value(model.get_variable_names()[1]))  # replace this line with your calculation\n",
    "    ### END CODE HERE ###\n",
    "    return theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 59\n",
      "linear/linear_model/bias_weights: [ 0.99782407]\n",
      "linear/linear_model/x/weights: [[ 1.0029664 ]\n",
      " [ 0.49972922]\n",
      " [ 0.19648392]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99782407,  1.0029664 ,  0.49972922,  0.19648392], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "theta_tf = tf_lin_regress(X_train, Y_train)\n",
    "theta_tf.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: run_normal_eq\n",
    "def run_normal_eq(X_train, Y_train, X_test, Y_test, learning_rate=0.05):\n",
    "    \"\"\"\n",
    "    Implements normal equation using tensorflow, trains the model using training data set\n",
    "    Tests the model quality by computing mean square error (MSE) of the test data set\n",
    "    \n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    X_test  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_test - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    \n",
    "    Return a tuple of:\n",
    "        - np.array of size (k+1 by 1) of regression coefficients\n",
    "        - mean square error (MSE) of the test data set\n",
    "        - mean square error (MSE) of the training data set\n",
    "    \"\"\"\n",
    "    ndim = X_train.shape[1]\n",
    "    lr_mse_train = 0.\n",
    "    lr_mse_test = 0.\n",
    "    ### START CODE HERE ### (≈ 20 lines of code)\n",
    "    \n",
    "\n",
    "    X = tf.constant(np.c_[np.ones((X_train.shape[0], 1)), X_train], dtype=tf.float32, name=\"X\")\n",
    "    y_train = tf.constant(Y_train, dtype=tf.float32, name=\"yhat\")\n",
    "    y_test = tf.constant(Y_test, dtype=tf.float32, name=\"y-test\")\n",
    "    X_test = tf.constant(np.c_[np.ones((X_test.shape[0], 1)), X_test], dtype=tf.float32, name=\"X-test\")\n",
    "    \n",
    "    XT = tf.transpose(X)\n",
    "    theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y_train)\n",
    "   \n",
    "    lr_mse_train = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.matmul(X,theta),y_train))))\n",
    "    \n",
    "    lr_mse_test = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.matmul(X_test,theta),y_test))))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        theta_value = theta.eval()\n",
    "        lr_mse_train = lr_mse_train.eval()\n",
    "        yhat_test = tf.matmul(X_test,theta_value)\n",
    "        lr_mse_test = lr_mse_test.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return theta_value, lr_mse_train, lr_mse_test\n",
    "\n",
    "### (DO NOT EDIT) ###\n",
    "theta_value, lr_mse_train, lr_mse_test = run_normal_eq(X_train, Y_train, X_test, Y_test)\n",
    "### (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00162387,  0.9981637 ,  0.50423741,  0.20078699], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "theta_value.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00162399  0.99816382  0.50423747  0.20078699] 0.00988956 0.00986643\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: run_mle\n",
    "def run_mle(X_train, Y_train, X_test, Y_test, learning_rate=0.05, num_iter=5000):\n",
    "    \"\"\"\n",
    "    Maximum likelihood Estimate (MLE)\n",
    "    Tests the model quality by computing mean square error (MSE) of the test data set\n",
    "    \n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    X_test  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_test - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    \n",
    "    Return a tuple of:\n",
    "        - np.array of size (k+1 by 1) of regression coefficients\n",
    "        - mean square error (MSE) of the test data set\n",
    "        - mean square error (MSE) of the training data set\n",
    "    \"\"\"\n",
    "    # create an instance of the Linear Regression model class  \n",
    "    ndim = X_train.shape[1]\n",
    "    loss = 0.\n",
    "    std_model = 0.\n",
    "    ### START CODE HERE ### (≈ 20 lines of code)\n",
    "    # Placeholder that is fed input data.\n",
    "    X_in = tf.placeholder(tf.float32, [None, ndim], \"X_in\")\n",
    "    \n",
    "    #PLacehold that is fed observed results.\n",
    "    y_in = tf.placeholder(tf.float32, [None, 1], \"y_in\")\n",
    "    \n",
    "    #Specify model y= wX+b\n",
    "    w = tf.Variable(tf.random_normal((ndim, 1)), name=\"w\") \n",
    "    b = tf.Variable(tf.constant(0., shape=None), name=\"b\")\n",
    "    yhat = tf.add(tf.matmul(X_in,w), b, name=\"yhat\")\n",
    "    \n",
    "    # The loss function: we are minimizing square root of mean \n",
    "    loss_op = tf.reduce_mean(tf.square(tf.subtract(y_in, yhat)), name=\"loss\")\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.3).minimize(loss_op)\n",
    "    \n",
    "    #Test the fitted model\n",
    "    \n",
    "    y_test = tf.constant(Y_test, dtype=tf.float32, name=\"y-test\")\n",
    "    X_test = tf.constant( X_test, dtype=tf.float32, name=\"X-test\")\n",
    "    std_model = tf.reduce_mean(tf.square(tf.subtract(tf.add(tf.matmul(X_test,w),b),y_test)))\n",
    "       \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for step in range(1000):\n",
    "            loss, weights, bees, extra = sess.run([loss_op, w, b, train_op], feed_dict={\n",
    "              X_in: X_train, \n",
    "              y_in: Y_train\n",
    "            })\n",
    "        std_model = sess.run(std_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "    weights = np.insert(weights,0,bees)\n",
    "    print(weights, loss, std_model)\n",
    "    ### END CODE HERE ###\n",
    "    return weights, loss, std_model\n",
    "\n",
    "weights, loss, std_model = run_mle(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00162399,  0.99816382,  0.50423747,  0.20078699], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "weights.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "coursera": {
   "course_slug": "guided-tour-machine-learning-finance",
   "graded_item_id": "dX8oQ",
   "launcher_item_id": "7Z9sN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
